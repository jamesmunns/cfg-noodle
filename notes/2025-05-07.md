# An update on design choices

After a [recent refinement of requirements], some expectations around read/write caching have been
made more clear, which might influence the architectural decisions around the storage service.

[recent refinement of requirements]: https://github.com/jamesmunns/cfg-noodle/compare/cb0005e7d60cc2841b86c9f35de122abced54adb..fe1bde19307561578ba6adf21e4313a6eb49c565

Previously, the proposed model of operation was morally equivalent to something like:

```rust
static STORAGE: Mutex<HashMap<PathKey, CborData>>;
```
Where:

* `sequential-storage::Map` or `ekv` would provide the "HashMap"-like interface
* All reads are more or less immediate and would be loaded from the external flash device
    * for `s-s::Map`, some kind of Key caching would be used, to amortize the `O(n)` seek time
    * `ekv` seeks are `O(log(n))`, and does not have an explicit cache interface
* All writes are more or less immediate, and would lead to a write to occur on the external flash
    * for `s-s::Map`, multiple writes could waste SOME space (e.g. "non-debounced write cost"),
      but appending to the "current write cursor" is also possible, so small writes don't suffer a
      "low efficiency write" penalty
    * `ekv` is [currently susceptible] to BOTH "low efficiency writes" as well as "non-debounced write
      costs", if updates are not "batched" into page-sized write transactions.

[currently susceptible]: https://docs.rs/ekv/1.0.0/ekv/index.html#future-work

However some important notes were raised:

1. It is NOT important to try and minimize "resident" memory usage, e.g. only holding on to
   configuration data "ephemerally" when data is needed; and we would instead prefer to ensure that
   there is sufficient "cache" to store the current value of all values at all times, e.g. in some
   kind of static allocation. Additionally, it is desirable for this "hydration" of configuration
   data to happen "all at once", in the process of system boot, and to avoid needing to go back and
   read data at a later point.
2. It IS important to actively manage when "write flushes" occur, in order to manage flash lifetime,
   avoiding both "low efficiency writes" and "non-debounced write costs", while still allowing for
   explicit "potentially-non-optimal" flushes, for example when shutting down.

**The previously proposed plan does not address any of these notes yet.**

For note 1 - we'll discuss how to achieve this "right sized" cache, used when reading, as well
as estimates of boot time loading.

For note 2 - we'll discuss how to handle "explicit flush control".

## "Right sized" read cache and data immediacy

In general, it is expected that there will be multiple "downstream consumer crates" of this service,
which will each independently and dynamically (over multiple firmware versions) declare and use
their relevant non-volatile storage items.

One option is to not try and get this perfect: we can guess at a reasonable amount of caching
necessary, for example using `s-s::Map`'s `KeyCache` to overcome worst-case `O(n)` seeks, and ask
users to load their value immediately at boot, and hold on to that loaded value for forever,
never doing another read.

We care about this, because using the "order of magnitude" numbers we mentioned before:

* Performing `M=64` "full scans", worst case: **256-512ms** (sequential-storage with NO key cache)
* Performing 1x "full scan" (to hydrate the cache), and `M=64` "single reads": **12-24ms**
  (sequential-storage with >= 64 key cache slots)
* Performing `M=64` "quick scans": **41-82ms** (ekv)

In this example, ekv with no cache would be considered acceptable for the 100-500ms boot time.

In this example, `sequential-storage` with no key cache would be right at the edge, and slightly
beyond what is considered acceptable for the 100-500ms boot time. Therefore, using `s-s` we will
definitely need a cache, especially if our total number of storage items may grow beyond the
expected `M=64`.

Ideally, we would have some way of ensuring that we have enough `KeyCache` storage for the `M`
k:v pairs that our system has. This ensures we can do a single "cache hydration" pass, roughly
`O(read(128KiB))`, and then service `M` x `O(read(4KiB))` read requests. If we get this number
TOO LOW, we risk incurring an `O(read(128KiB))` cache miss penalty for EACH miss. If we get this
number TOO HIGH, we waste static RAM usage.

Currently, `sequential-storage`'s Key cache is **generic** over the number of keys we expect to
cache. This means that we need to calculate a `const` value for "the number of keys". I am not
aware of any way to perform this calculation in today's Rust.

The other common option for "global buffer size analysis" is using the linker. This doesn't allow
us to calculate a `const` value, but can be used to create a static buffer. We could potentially
implement a new kind of KeyCache that uses an externally defined array, and use link-time hacks to
create static storage for each record, and group the statically allocated keys into a
linker-arranged RAM array, unsafely grab this array, and use it to create a key cache that holds the
correct number of items. This space is well explored by the [`linkme` crate].

If our keys are approximately 16 bytes, and we need to store a pointer to where that key is
resident, that means our key cache will take an additional 64 x (16 + 4 bytes), or approximately
1.25KiB of storage for the key cache.

[`linkme` crate]: https://docs.rs/linkme/latest/linkme/

This means that the "right sizing" is solvable (with a new kind of key cache impl), though we are
currently still requiring "ask users to load their value immediately at boot, and hold on to that
loaded value for forever, never doing another explicit read.", which is fine, but does require us
to verify this behavior at code review time or "by convention". We would not generally use the
key cache after this "initial hydration" is complete, which is a bit of a waste.

**Overall**: on this one criteria, `ekv` is an "easy win", while `s-s` has "mitigable edge cases",
with reasonably limited `unsafe` code and new development.

## Managing "write flushes"

We want to avoid excess writes, including both "low efficiency writes" and "non-debounced write
costs".

`ekv` currently provides NO way of amortizing the cost of flash writes: a single small write will
erase and write an entire page. non-debounced writes will repeat this *for every write*. This means
for this to be practical for our use case, we will need to put some kind of write cache in front
of `ekv`, batching writes until an "immediate flush" is commanded, or we reach a "full page" write
threshold.

`sequential-storage` DOES amortize the cost of small writes by allowing multiple writes to the
same page, but DOES NOT mitigate multiple writes of the same data. To support this, it would also
need some kind of write cache, in addition to the previously discussed

This means that we would need to have an additional `[u8; 4096]` or so of buffer coupled to the
storage in order to hold up to one page's worth of data, allowing us to batch, and de-duplicate
repeated writes within the same window.

However, if we still "ask users to load their value immediately at boot, and hold on to that loaded
value for forever, never doing another explicit read", we now require them to do THREE things:

* Make sure they load the value on boot
* Hold on to the value
* WHENEVER THEY CHANGE the value, make sure to inform the storage of this change, so it can be
  written to flash at some later point.

The second and third points can likely be somewhat mitigated with the right library interface.
Something like:

```rust
// If we need to use `linkme` for calculating relevant KeyCache, this might have to be more of
// a macro, that creates a `#[distributed_slice] for the key, and a normal static for the handle.
//
// maybe something like `storage_cell!("encabulator/polarity": EncabulatorSettings);`
static CONFIG: StorageCell<EncabulatorSettings> = StorageCell::new_with_key(
    "encabulator/polarity"
);

// Here `GlobalStorageHandle` is "something" that holds the underlying flash storage interface
// that performs flash access, etc.
#[task]
async fn encabulator(storage: &'static GlobalStorageHandle) {
    // This line:
    //
    // 1. Makes sure that we only take `CONFIG` once, like `StaticCell` or `OnceCell`, either
    //    "once ever", or "one at a time"
    // 2. Stores the shared ref to the global, which is only initialized after boot, which the
    //    `StorageCell` can use later
    // 3. That the value has been loaded from flash, or is initialized with some default value
    let config = CONFIG.take_with(storage).await;

    // Later, we can access the data we've loaded:
    let data: &EncabulatorSettings = config.deref();

    // We can also get mutable access, with some interface that prompts users to report-back their
    // changes
    let mut write_guard = config.get_mut();
    write_guard.data = /* new data */
    // If they don't call `commit`, the data isn't written back to flash! This goes out to the
    // flash handler referenced by GlobalStorageHandle, and:
    //
    // 1. gets a write lock on the "write cache"
    // 2. serializes the data to the write cache
    //     * If there WAS room for this, then no data is flushed to the cache, will be done later
    //     * If there WAS NOT room for this, then the currently pending data will be flushed,
    //       and the currently changed data will be written to the newly empty write cache
    write_guard.commit().await;
}
```

**Overall**: If we **require** de-bouncing of writes, and **require** explicit control of when
writes occur, then we need an additional 4KiB of write storage for the write cache. This would be
additional work for both `ekv` and `sequential-storage`, and required by both.

## This is feeling suboptimal

At this point, we either need to:

* Switch from the previous plan of using `sequential-storage::Map` to `ekv`, AND budget for an
  extra 4KiB of write cache, as well as the development effort to integrate it.
* Stick with `sequential-storage::Map`, and:
    * add roughly 5.25KiB of Key and Write caches, implement both kinds of caches
    * Implement a new kind of KeyCache, using "linker hacks"
    * Implement a way to do a "cache hydration pass" for the new KeyCache

For storing a total of 4KiB of value data, both of these options feel somewhat suboptimal.

TODO: tomorrow I'll expand on this. I think a better option is to:

* Use the intrusive scheme, described below
    * The "write cache" is stored in locally, we can probably get away with storing a single
      non-serialized copy in the `StoreCell`, depending on whether we want `DerefMut`-like access,
      or whether we expect users to copy-out with mutex access.
    * There's no need to immediately flush when we have 4KiB of pending writes, we can do it
      whenever we want to
    * Note to self: we should keep the intrusive list sorted, in case we need that for `ekv`
    * Discuss the risks around "this doesn't exist yet!" and "lots of unsafe"
* As for storage:
    * We could use `ekv`, now that we have somewhere to store pending writes (in their intrusive
      node)
    * OR, if we KNOW that the data, including keys, probably fits in 1-2 pages, maybe we just use
      something like `sequential-storage::Queue`, and ALWAYS write ALL values, which means lookups
      are always `O(1)`, after an initial `O(n)` "cursor scan"

# An intrusive model for settings?

```rust
use store_cell::{StoreCell, GLOBAL_LIST, StoreWorker};
static SETTINGS: StoreCell<Settings> = StoreCell::with_path("encabulator/polarity");

#[task]
async fn encabulator(...) {
    // The library could provide the "anchor point", e.g. the `GLOBAL_LIST`, or
    // we could make users provide the list themselves. Not sure which is
    // preferrable. With a global list we could do this transparently, but we would
    // be less flexible if users wanted to split storage to multiple different
    // partitions of their choice.
    //
    // One of these:
    //
    // SETTINGS.attach_global();
    // SETTINGS.attach(passed_in_list_reference)
    // SETTINGS.attach(project::CONFIG_LIST);
    // SETTINGS.attach(&GLOBAL_LIST).unwrap();
    //
    // This can error if there is a duplicate key? Maybe if already attached?
    //
    // This attaches the local settings object to the global linked list of items
    //
    // TODO: How to handle "borrowed" types like &str? Do we care about this?
    SETTINGS.attach(&GLOBAL_LIST).unwrap();

    // On the first call, this realizes the storage is empty, and awaits a read.
    // If the load fails, we store the default value, and marks the storage
    // block as "dirty" (maybe? should we just use default and NOT store-back?)
    //
    // This will yield until the first time the "storage worker" task attempts to
    // process all requests. If the "storage worker" is already awaiting notifications,
    // it will wake the worker.
    //
    // Ideally we never offer a `Deref`/`DerefMut` impl, this would require locking
    // a mutex to access to inhibit the storage_worker. This might be a little more
    // relaxed than PinList if we mandate static storage. Copy data out.
    let settings: Settings = SETTINGS.load_or_default().await;

    loop {
        Timer::after_seconds(60 * 60).await;
        let new_settings: Settings = ...;

        // Update the settings with a new value. This is NOT written immediately,
        // but does set a "needs writing" flag in the container and wakes the
        // storage_worker if it's awaiting a "write needed" notification.
        SETTINGS.write(new_settings);
    }
}

// I guess for the global list, this could be a provided fn you could just call in a task?
#[task]
async fn storage_worker(flash: ...) {
    // Create our new driver. Probably initializes flash if opening the
    // ekv storage fails
    let driver = StoreWorker::new(flash, &GLOBAL_LIST);

    // read/write cache
    // todo maybe storage for N node ptrs? store them inline?
    let mut buf = [0u8; 4096];

    // We actually want to stall a little bit to give all the other tasks a chance
    // to attach. This means that when we make the first load pass, all the
    // "needs to be hydrated" StoreCells are ready to be loaded.
    Timer::after_millis(10).await;

    // This is our basic runloop, vaguely
    loop {
        // Wait for some task to need a read or a write. Registers the listener,
        // then does one pass to see if anything is immediately ready. Takes the
        // mutex to do this.
        let (needs_read, needs_write) = driver.poll_activity().await;

        // TODO: We only need to read/hydrate at boot, unless we want some kind of "reload" interface
        if needs_read {
            // Starts an ekv read transaction, then fills any "needs read" StoreCell
            // that has been attached
            //
            // Can fail on some kind of flash access error?
            driver.process_reads(&mut buf).await.unwrap();
        }

        // TODO: Set some minimum time between writes? We probably want some kind of way
        // to batch writes when some combination of the following are true:
        //
        // * There's a "go now" command, e.g. when we're going to reboot soon
        // * Some time-delay interval, maybe only started after the first "dirty" page, e.g. debouncing
        // * There is a full page worth of writes ready to be written
        if needs_write {
            // Locks the list, walks nodes that need a write, and serializes as many as fit in `&mut buf`.
            //
            // TODO: We probably want to make sure that `pending` marks all writes failed on a cancellation
            if let Some(mut pending) = driver.consume_writes(&mut buf) {
                let mut txn = driver.write_transaction().await;
                // TODO: This iteration needs to be in sorted order!
                for p in pending.iter_mut() {
                    let res = txn.write(p.key, p.value).await;
                    if res.is_ok() {
                        // Update the node that writing has started
                        p.mark_writing();
                    } else {
                        // Update the node that writing has not occurred
                        // todo store the actual error? just retry later?
                        p.mark_failed();
                    }
                }

                let res = txn.commit().await;
                if res.is_err() {
                    p.mark_all_failed();
                } else {
                    p.all_writing_to_written();
                }
            }
        }
    }
}
```
