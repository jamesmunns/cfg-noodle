# An intrusive model for settings?

```rust
use store_cell::{StoreCell, GLOBAL_LIST, StoreWorker};
static SETTINGS: StoreCell<Settings> = StoreCell::with_path("encabulator/polarity");

#[task]
async fn encabulator(...) {
    // The library could provide the "anchor point", e.g. the `GLOBAL_LIST`, or
    // we could make users provide the list themselves. Not sure which is
    // preferrable. With a global list we could do this transparently, but we would
    // be less flexible if users wanted to split storage to multiple different
    // partitions of their choice.
    //
    // One of these:
    //
    // SETTINGS.attach_global();
    // SETTINGS.attach(passed_in_list_reference)
    // SETTINGS.attach(project::CONFIG_LIST);
    // SETTINGS.attach(&GLOBAL_LIST).unwrap();
    //
    // This can error if there is a duplicate key? Maybe if already attached?
    //
    // This attaches the local settings object to the global linked list of items
    //
    // TODO: How to handle "borrowed" types like &str? Do we care about this?
    SETTINGS.attach(&GLOBAL_LIST).unwrap();

    // On the first call, this realizes the storage is empty, and awaits a read.
    // If the load fails, we store the default value, and marks the storage
    // block as "dirty" (maybe? should we just use default and NOT store-back?)
    //
    // This will yield until the first time the "storage worker" task attempts to
    // process all requests. If the "storage worker" is already awaiting notifications,
    // it will wake the worker.
    //
    // Ideally we never offer a `Deref`/`DerefMut` impl, this would require locking
    // a mutex to access to inhibit the storage_worker. This might be a little more
    // relaxed than PinList if we mandate static storage. Copy data out.
    let settings: Settings = SETTINGS.load_or_default().await;

    loop {
        Timer::after_seconds(60 * 60).await;
        let new_settings: Settings = ...;

        // Update the settings with a new value. This is NOT written immediately,
        // but does set a "needs writing" flag in the container and wakes the
        // storage_worker if it's awaiting a "write needed" notification.
        SETTINGS.write(new_settings);
    }
}

// I guess for the global list, this could be a provided fn you could just call in a task?
#[task]
async fn storage_worker(flash: ...) {
    // Create our new driver. Probably initializes flash if opening the
    // ekv storage fails
    let driver = StoreWorker::new(flash, &GLOBAL_LIST);

    // read/write cache
    // todo maybe storage for N node ptrs? store them inline?
    let mut buf = [0u8; 4096];

    // We actually want to stall a little bit to give all the other tasks a chance
    // to attach. This means that when we make the first load pass, all the
    // "needs to be hydrated" StoreCells are ready to be loaded.
    Timer::after_millis(10).await;

    // This is our basic runloop, vaguely
    loop {
        // Wait for some task to need a read or a write. Registers the listener,
        // then does one pass to see if anything is immediately ready. Takes the
        // mutex to do this.
        let (needs_read, needs_write) = driver.poll_activity().await;

        // TODO: We only need to read/hydrate at boot, unless we want some kind of "reload" interface
        if needs_read {
            // Starts an ekv read transaction, then fills any "needs read" StoreCell
            // that has been attached
            //
            // Can fail on some kind of flash access error?
            driver.process_reads(&mut buf).await.unwrap();
        }

        // TODO: Set some minimum time between writes? We probably want some kind of way
        // to batch writes when some combination of the following are true:
        //
        // * There's a "go now" command, e.g. when we're going to reboot soon
        // * Some time-delay interval, maybe only started after the first "dirty" page, e.g. debouncing
        // * There is a full page worth of writes ready to be written
        if needs_write {
            // Locks the list, walks nodes that need a write, and serializes as many as fit in `&mut buf`.
            //
            // TODO: We probably want to make sure that `pending` marks all writes failed on a cancellation
            if let Some(mut pending) = driver.consume_writes(&mut buf) {
                let mut txn = driver.write_transaction().await;
                // TODO: This iteration needs to be in sorted order!
                for p in pending.iter_mut() {
                    let res = txn.write(p.key, p.value).await;
                    if res.is_ok() {
                        // Update the node that writing has started
                        p.mark_writing();
                    } else {
                        // Update the node that writing has not occurred
                        // todo store the actual error? just retry later?
                        p.mark_failed();
                    }
                }

                let res = txn.commit().await;
                if res.is_err() {
                    p.mark_all_failed();
                } else {
                    p.all_writing_to_written();
                }
            }
        }
    }

}
